# ECAP Logstash Pipeline Configuration
# Processes structured logs from ECAP services

input {
  # Filebeat input for log files
  beats {
    port => 5044
    host => "0.0.0.0"
  }

  # TCP input for direct log shipping
  tcp {
    port => 5000
    codec => json_lines
    add_field => { "input_type" => "tcp" }
  }

  # UDP input for high-volume logs
  udp {
    port => 5000
    codec => json_lines
    add_field => { "input_type" => "udp" }
  }

  # HTTP input for webhook-style log delivery
  http {
    port => 8080
    codec => json
    add_field => { "input_type" => "http" }
  }
}

filter {
  # Parse timestamp field
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }

  # Parse @timestamp field (ECS format)
  if [@timestamp] and [@timestamp] != "" {
    date {
      match => [ "@timestamp", "ISO8601" ]
    }
  }

  # Add processing metadata
  mutate {
    add_field => {
      "processed_at" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}"
      "pipeline_version" => "1.0.0"
    }
  }

  # Enrich service information
  if [service] {
    if [service][name] {
      mutate {
        add_field => { "service_name" => "%{[service][name]}" }
      }
    }
    if [service][environment] {
      mutate {
        add_field => { "environment" => "%{[service][environment]}" }
      }
    }
  }

  # Process HTTP logs
  if [http] {
    # Calculate response categories
    if [http][response][status_code] {
      if [http][response][status_code] >= 500 {
        mutate { add_field => { "response_category" => "5xx_server_error" } }
      } else if [http][response][status_code] >= 400 {
        mutate { add_field => { "response_category" => "4xx_client_error" } }
      } else if [http][response][status_code] >= 300 {
        mutate { add_field => { "response_category" => "3xx_redirect" } }
      } else if [http][response][status_code] >= 200 {
        mutate { add_field => { "response_category" => "2xx_success" } }
      } else {
        mutate { add_field => { "response_category" => "1xx_informational" } }
      }
    }

    # Classify request duration
    if [http][duration_ms] {
      if [http][duration_ms] > 5000 {
        mutate { add_field => { "performance_category" => "very_slow" } }
      } else if [http][duration_ms] > 2000 {
        mutate { add_field => { "performance_category" => "slow" } }
      } else if [http][duration_ms] > 1000 {
        mutate { add_field => { "performance_category" => "acceptable" } }
      } else {
        mutate { add_field => { "performance_category" => "fast" } }
      }
    }
  }

  # Process database logs
  if [database] {
    # Classify query performance
    if [database][duration_ms] {
      if [database][duration_ms] > 5000 {
        mutate { add_field => { "db_performance" => "very_slow" } }
      } else if [database][duration_ms] > 1000 {
        mutate { add_field => { "db_performance" => "slow" } }
      } else if [database][duration_ms] > 500 {
        mutate { add_field => { "db_performance" => "acceptable" } }
      } else {
        mutate { add_field => { "db_performance" => "fast" } }
      }
    }

    # Extract query type
    if [database][query] {
      if [database][query] =~ /^SELECT/ {
        mutate { add_field => { "query_type" => "select" } }
      } else if [database][query] =~ /^INSERT/ {
        mutate { add_field => { "query_type" => "insert" } }
      } else if [database][query] =~ /^UPDATE/ {
        mutate { add_field => { "query_type" => "update" } }
      } else if [database][query] =~ /^DELETE/ {
        mutate { add_field => { "query_type" => "delete" } }
      } else {
        mutate { add_field => { "query_type" => "other" } }
      }
    }
  }

  # Process Kafka logs
  if [kafka] {
    # Add Kafka enrichment
    mutate {
      add_field => { "message_broker" => "kafka" }
    }

    # Classify event types
    if [kafka][event_type] {
      if [kafka][event_type] in ["producer_send", "producer_ack"] {
        mutate { add_field => { "kafka_operation" => "produce" } }
      } else if [kafka][event_type] in ["consumer_fetch", "consumer_commit"] {
        mutate { add_field => { "kafka_operation" => "consume" } }
      } else {
        mutate { add_field => { "kafka_operation" => "admin" } }
      }
    }
  }

  # Error log processing
  if [level] in ["ERROR", "CRITICAL", "error", "critical"] {
    mutate {
      add_field => { "alert_required" => "true" }
    }

    # Extract error patterns
    if [message] {
      if [message] =~ /timeout/i {
        mutate { add_field => { "error_category" => "timeout" } }
      } else if [message] =~ /connection/i {
        mutate { add_field => { "error_category" => "connection" } }
      } else if [message] =~ /authentication|authorization/i {
        mutate { add_field => { "error_category" => "auth" } }
      } else if [message] =~ /validation|invalid/i {
        mutate { add_field => { "error_category" => "validation" } }
      } else {
        mutate { add_field => { "error_category" => "unknown" } }
      }
    }
  }

  # Security log processing
  if [level] in ["WARNING", "ERROR"] and [message] =~ /suspicious|attack|unauthorized|forbidden/i {
    mutate {
      add_field => { "security_event" => "true" }
    }
  }

  # Geographic enrichment for client IPs
  if [client_ip] and [client_ip] != "unknown" and [client_ip] !~ /^(127\.|10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[01])\.)/  {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }

  # User-Agent parsing for HTTP requests
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
    }
  }

  # Clean up and normalize
  mutate {
    # Remove empty fields
    remove_field => ["[ua][patch]", "beat", "prospector", "input", "source", "offset"]

    # Convert numeric strings to numbers
    convert => {
      "[http][response][status_code]" => "integer"
      "[http][duration_ms]" => "float"
      "[database][duration_ms]" => "float"
      "[database][rows_affected]" => "integer"
      "[kafka][partition]" => "integer"
      "[kafka][offset]" => "integer"
    }

    # Lowercase certain fields
    lowercase => ["level", "environment", "service_name"]
  }
}

output {
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "ecap-logs-%{+YYYY.MM.dd}"
    template_name => "ecap-logs"
    template_pattern => "ecap-logs-*"
    template => "/usr/share/logstash/templates/ecap-logs-template.json"
    template_overwrite => true

    # Document ID for deduplication
    document_id => "%{correlation_id}_%{[@timestamp]}"

    # Retry failed documents
    retry_on_conflict => 3
  }

  # Separate index for error logs
  if [level] in ["ERROR", "CRITICAL", "error", "critical"] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ecap-errors-%{+YYYY.MM.dd}"
      template_name => "ecap-errors"
      template_pattern => "ecap-errors-*"
      template => "/usr/share/logstash/templates/ecap-errors-template.json"
      template_overwrite => true
    }
  }

  # High-performance logs to separate index
  if [http][duration_ms] and [http][duration_ms] > 2000 {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ecap-performance-%{+YYYY.MM.dd}"
    }
  }

  # Security events to separate index
  if [security_event] == "true" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ecap-security-%{+YYYY.MM.dd}"
    }
  }

  # Debug output (uncomment for troubleshooting)
  # stdout { codec => rubydebug }
}
