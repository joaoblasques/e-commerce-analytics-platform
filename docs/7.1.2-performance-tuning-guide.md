# E-Commerce Analytics Platform - Performance Tuning Guide

## Overview

This document provides comprehensive performance tuning guidance for the E-Commerce Analytics Platform (ECAP). It covers optimization techniques implemented throughout the system architecture, infrastructure scaling strategies, capacity planning methodologies, and cost optimization approaches based on real-world performance testing and production deployment experience.

## Table of Contents

1. [Apache Spark Optimization Techniques](#apache-spark-optimization-techniques)
2. [Infrastructure Scaling Guidelines](#infrastructure-scaling-guidelines)
3. [Capacity Planning Documentation](#capacity-planning-documentation)
4. [Cost Optimization Strategies](#cost-optimization-strategies)
5. [Performance Monitoring and Alerting](#performance-monitoring-and-alerting)
6. [Troubleshooting Performance Issues](#troubleshooting-performance-issues)

## Apache Spark Optimization Techniques

### Core Configuration Optimizations

#### Adaptive Query Execution (AQE)
```yaml
# Enabled by default in src/utils/spark_utils.py
spark.sql.adaptive.enabled: true
spark.sql.adaptive.coalescePartitions.enabled: true
```

**Benefits:**
- Dynamic partition coalescing reduces small file problems
- Automatic skew handling improves join performance
- Query optimization based on runtime statistics

#### Serialization Optimization
```yaml
# KryoSerializer for better performance
spark.serializer: org.apache.spark.serializer.KryoSerializer
```

**Performance Impact:**
- 3-10x faster serialization compared to Java serialization
- Reduced memory footprint for cached data
- Lower network I/O for shuffle operations

#### Arrow Integration
```yaml
# Enabled for faster DataFrame operations
spark.sql.execution.arrow.pyspark.enabled: true
```

**Benefits:**
- 10-100x faster conversion between Spark and Pandas DataFrames
- Reduced memory usage for UDF operations
- Vectorized operations for improved performance

### Memory Management

#### Dynamic Allocation Configuration
```yaml
# Production EMR configuration (terraform/modules/emr/main.tf)
spark.dynamicAllocation.enabled: true
spark.dynamicAllocation.minExecutors: 2
spark.dynamicAllocation.maxExecutors: 20
spark.dynamicAllocation.initialExecutors: 4
spark.executor.memory: 4g
spark.executor.memoryFraction: 0.8
spark.executor.cores: 2
```

**Tuning Guidelines:**
- **Memory per Executor**: Use 4-8GB for analytics workloads
- **Cores per Executor**: Keep at 2-5 to avoid GC pressure
- **Memory Fraction**: 0.6-0.8 for storage, rest for execution

#### Garbage Collection Tuning
```bash
# EMR configuration for production workloads
export SPARK_WORKER_OPTS="-XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+UseStringDeduplication"
export SPARK_EXECUTOR_OPTS="-XX:+UseG1GC -XX:G1HeapRegionSize=16m"
```

### Data Processing Optimizations

#### Partitioning Strategy
```python
# Implemented in src/data_lake/storage.py
def optimize_partitions(df, target_partition_size_mb=128):
    """
    Optimize DataFrame partitions for performance.
    Target: 128MB per partition for optimal HDFS block size.
    """
    total_size_mb = df.rdd.map(lambda x: len(str(x))).sum() / 1024 / 1024
    optimal_partitions = max(1, int(total_size_mb / target_partition_size_mb))
    return df.coalesce(optimal_partitions)
```

#### Join Optimizations
```python
# Broadcast join threshold (config/default.yaml)
spark.sql.autoBroadcastJoinThreshold: 50MB

# Implemented join strategy in analytics modules
def optimize_join(large_df, small_df):
    if small_df.count() < 1000000:  # < 50MB estimated
        return large_df.join(broadcast(small_df), "key")
    else:
        return large_df.join(small_df.hint("shuffle_hash"), "key")
```

#### Caching Strategy
```python
# Implemented in src/analytics/rfm_segmentation.py
def cache_frequently_accessed_data(df):
    """
    Cache DataFrames that are accessed multiple times.
    Use appropriate storage levels based on memory availability.
    """
    if df.rdd.getNumPartitions() > 200:
        return df.persist(StorageLevel.MEMORY_AND_DISK_SER)
    else:
        return df.persist(StorageLevel.MEMORY_ONLY_SER)
```

### Streaming Optimizations

#### Kafka Consumer Configuration
```yaml
# config/default.yaml - Optimized for throughput
kafka:
  consumer:
    batch_size: 1000
    max_poll_records: 500
    session_timeout_ms: 30000
    heartbeat_interval_ms: 3000
```

#### Structured Streaming Settings
```python
# src/streaming/consumer_manager.py
streaming_config = {
    "spark.sql.streaming.checkpointInterval": "10 seconds",
    "spark.sql.streaming.minBatchesToRetain": 10,
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true"
}
```

**Performance Targets:**
- **Throughput**: >1000 events/second per partition
- **Latency**: <5 seconds end-to-end processing
- **Checkpoint Interval**: 10-60 seconds based on workload

## Infrastructure Scaling Guidelines

### AWS EMR Scaling Strategy

#### Instance Type Selection
```hcl
# terraform/modules/emr/main.tf
variable "instance_groups" {
  default = {
    master = {
      instance_type  = "m5.xlarge"
      instance_count = 1
    }
    core = {
      instance_type  = "m5.2xlarge"
      instance_count = 2
      ebs_config = {
        size = 100
        type = "gp3"
        throughput = 250
      }
    }
    task = {
      instance_type  = "m5.xlarge"
      instance_count = 0
      market_type    = "SPOT"
      bid_price      = "0.10"
    }
  }
}
```

#### Auto Scaling Configuration
```hcl
# EMR Auto Scaling Rules
resource "aws_emr_managed_scaling_policy" "example" {
  cluster_id = aws_emr_cluster.cluster.id

  compute_limits {
    unit_type                       = "Instances"
    minimum_capacity_units          = 2
    maximum_capacity_units          = 20
    maximum_ondemand_capacity_units = 5
    maximum_core_capacity_units     = 10
  }
}
```

**Scaling Triggers:**
- **Scale Up**: CPU > 70% for 5 minutes OR Memory > 80%
- **Scale Down**: CPU < 30% for 10 minutes AND Memory < 50%
- **Maximum Scale**: 20 instances (cost protection)

### Kubernetes Scaling

#### Horizontal Pod Autoscaler (HPA)
```yaml
# helm/templates/api-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### Vertical Pod Autoscaler (VPA)
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: api
      maxAllowed:
        cpu: 2
        memory: 4Gi
      minAllowed:
        cpu: 100m
        memory: 256Mi
```

### Database Scaling

#### PostgreSQL Configuration
```yaml
# RDS Parameter Group (terraform/modules/rds/main.tf)
parameters = [
  {
    name  = "shared_preload_libraries"
    value = "pg_stat_statements"
  },
  {
    name  = "max_connections"
    value = "200"
  },
  {
    name  = "shared_buffers"
    value = "{DBInstanceClassMemory/4}"
  },
  {
    name  = "effective_cache_size"
    value = "{DBInstanceClassMemory*3/4}"
  },
  {
    name  = "maintenance_work_mem"
    value = "256MB"
  },
  {
    name  = "checkpoint_completion_target"
    value = "0.7"
  },
  {
    name  = "wal_buffers"
    value = "16MB"
  }
]
```

#### Connection Pooling
```python
# src/database/config.py
DATABASE_CONFIG = {
    "pool_size": 10,
    "max_overflow": 20,
    "pool_timeout": 30,
    "pool_recycle": 3600,
    "pool_pre_ping": True
}
```

### Redis Scaling

#### Cluster Configuration
```hcl
# terraform/modules/elasticache/main.tf
resource "aws_elasticache_replication_group" "redis" {
  replication_group_id       = "${var.project_name}-redis"
  description                = "Redis cluster for caching"

  node_type                  = "cache.r6g.large"
  port                       = 6379
  parameter_group_name       = "default.redis7"

  num_cache_clusters         = 3
  automatic_failover_enabled = true
  multi_az_enabled          = true

  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
}
```

## Capacity Planning Documentation

### Resource Calculation Methodology

#### Spark Cluster Sizing
```python
def calculate_spark_resources(daily_data_gb, processing_hours=8):
    """
    Calculate optimal Spark cluster resources based on data volume.

    Args:
        daily_data_gb: Daily data volume in GB
        processing_hours: Available processing window in hours

    Returns:
        dict: Resource recommendations
    """
    # Rule of thumb: 1GB raw data requires 2-4GB memory for processing
    memory_multiplier = 3
    required_memory_gb = daily_data_gb * memory_multiplier

    # Target: Process data in 25% of available window for buffer
    target_hours = processing_hours * 0.25

    # Each executor can process ~10GB/hour with 4GB memory
    throughput_per_executor = 10  # GB/hour
    required_executors = max(2, int(daily_data_gb / (throughput_per_executor * target_hours)))

    # Memory per executor (4-8GB optimal)
    memory_per_executor = min(8, max(4, required_memory_gb / required_executors))

    # Adjust executor count based on memory
    final_executors = int(required_memory_gb / memory_per_executor)

    return {
        "executors": final_executors,
        "memory_per_executor": f"{memory_per_executor}g",
        "cores_per_executor": 2,
        "total_memory_gb": final_executors * memory_per_executor,
        "estimated_cost_per_hour": final_executors * 0.20  # ~$0.20 per executor-hour
    }

# Example calculations:
# 100GB daily: 34 executors, 6GB each, ~$6.80/hour
# 500GB daily: 84 executors, 6GB each, ~$16.80/hour
# 1TB daily: 167 executors, 6GB each, ~$33.40/hour
```

#### Database Sizing
```python
def calculate_database_resources(transactions_per_day, retention_days=365):
    """
    Calculate PostgreSQL resource requirements.

    Transaction data model:
    - Average transaction size: ~2KB
    - Index overhead: 30%
    - Analytics tables: 2x transaction data
    """
    # Storage calculation
    transaction_size_kb = 2
    index_overhead = 1.3
    analytics_multiplier = 2

    daily_storage_gb = (transactions_per_day * transaction_size_kb * index_overhead) / 1024 / 1024
    total_storage_gb = daily_storage_gb * retention_days * analytics_multiplier

    # Memory calculation (25% of data size, min 16GB)
    required_memory_gb = max(16, total_storage_gb * 0.25)

    # Instance sizing
    if required_memory_gb <= 32:
        instance_type = "db.r6g.xlarge"
        cost_per_hour = 0.504
    elif required_memory_gb <= 64:
        instance_type = "db.r6g.2xlarge"
        cost_per_hour = 1.008
    else:
        instance_type = "db.r6g.4xlarge"
        cost_per_hour = 2.016

    return {
        "storage_gb": int(total_storage_gb * 1.2),  # 20% buffer
        "memory_gb": required_memory_gb,
        "instance_type": instance_type,
        "estimated_monthly_cost": cost_per_hour * 24 * 30
    }
```

#### Kafka Sizing
```python
def calculate_kafka_resources(events_per_second, retention_hours=168):
    """
    Calculate Kafka cluster requirements.

    Event sizing:
    - Transaction events: ~1KB
    - User behavior events: ~0.5KB
    - Replication factor: 3
    """
    # Throughput calculation
    avg_event_size_kb = 0.75
    replication_factor = 3

    daily_events = events_per_second * 86400
    daily_storage_gb = (daily_events * avg_event_size_kb * replication_factor) / 1024 / 1024

    retention_days = retention_hours / 24
    total_storage_gb = daily_storage_gb * retention_days

    # Partition calculation (max 50MB per partition per day)
    max_partition_size_mb = 50
    partitions_needed = max(12, int(daily_storage_gb * 1024 / max_partition_size_mb))

    # Broker count (min 3 for HA)
    brokers_needed = max(3, int(partitions_needed / 10))  # ~10 partitions per broker

    return {
        "brokers": brokers_needed,
        "partitions_per_topic": partitions_needed,
        "storage_per_broker_gb": int(total_storage_gb / brokers_needed * 1.3),  # 30% buffer
        "instance_type": "kafka.m5.large",
        "estimated_monthly_cost": brokers_needed * 0.192 * 24 * 30
    }
```

### Growth Planning

#### Traffic Growth Projections
```python
GROWTH_SCENARIOS = {
    "conservative": {
        "yearly_growth": 0.50,  # 50% YoY growth
        "seasonal_peak": 2.0,   # 2x traffic during peak season
        "description": "Steady growth, established market"
    },
    "aggressive": {
        "yearly_growth": 1.0,   # 100% YoY growth
        "seasonal_peak": 3.0,   # 3x traffic during peak season
        "description": "High growth startup, expanding market"
    },
    "enterprise": {
        "yearly_growth": 0.25,  # 25% YoY growth
        "seasonal_peak": 1.5,   # 1.5x traffic during peak season
        "description": "Mature enterprise, stable growth"
    }
}

def project_capacity_needs(current_daily_transactions, years=3, scenario="conservative"):
    """Project future capacity requirements."""
    growth_rate = GROWTH_SCENARIOS[scenario]["yearly_growth"]
    seasonal_peak = GROWTH_SCENARIOS[scenario]["seasonal_peak"]

    projections = []
    for year in range(1, years + 1):
        base_transactions = current_daily_transactions * ((1 + growth_rate) ** year)
        peak_transactions = base_transactions * seasonal_peak

        projections.append({
            "year": year,
            "base_daily_transactions": int(base_transactions),
            "peak_daily_transactions": int(peak_transactions),
            "spark_resources": calculate_spark_resources(peak_transactions / 100000),  # Convert to GB
            "database_resources": calculate_database_resources(peak_transactions),
            "kafka_resources": calculate_kafka_resources(peak_transactions / 86400)  # Events per second
        })

    return projections
```

## Cost Optimization Strategies

### Spot Instance Utilization

#### EMR Spot Configuration
```hcl
# terraform/modules/emr/main.tf - 60% cost savings
resource "aws_emr_instance_group" "task" {
  cluster_id     = aws_emr_cluster.cluster.id
  instance_group_id = aws_emr_cluster.cluster.core_instance_group[0].id

  name           = "task"
  instance_type  = "m5.2xlarge"
  instance_count = 0

  configurations_json = jsonencode([
    {
      "classification" = "yarn-site"
      "properties" = {
        "yarn.resourcemanager.decommissioning.timeout" = "300"
        "yarn.nodemanager.graceful-decommission-timeout-secs" = "300"
      }
    }
  ])

  bid_price = "0.15"  # ~70% discount from on-demand

  ebs_config {
    size = 100
    type = "gp3"
    volumes_per_instance = 1
    throughput = 250
  }

  auto_scaling_policy = jsonencode({
    Constraints = {
      MinCapacity = 0
      MaxCapacity = 10
    }
    Rules = [
      {
        Name = "ScaleOutMemoryPercentage"
        Description = "Scale out if YARNMemoryAvailablePercentage is less than 30"
        Action = {
          Market = "SPOT"
          SimpleScalingPolicyConfiguration = {
            AdjustmentType = "CHANGE_IN_CAPACITY"
            ScalingAdjustment = 2
            CoolDown = 300
          }
        }
        Trigger = {
          CloudWatchAlarmDefinition = {
            ComparisonOperator = "LESS_THAN"
            EvaluationPeriods = 2
            MetricName = "YARNMemoryAvailablePercentage"
            Namespace = "AWS/ElasticMapReduce"
            Period = 300
            Statistic = "AVERAGE"
            Threshold = 30.0
            Unit = "PERCENT"
          }
        }
      }
    ]
  })
}
```

### Reserved Instance Strategy
```python
RESERVED_INSTANCE_RECOMMENDATIONS = {
    "database": {
        "commitment": "1_year",
        "payment": "all_upfront",
        "savings": "42%",
        "rationale": "Predictable always-on workload"
    },
    "baseline_compute": {
        "commitment": "1_year",
        "payment": "partial_upfront",
        "coverage": "50%",  # Cover baseline, use spot for peaks
        "savings": "35%"
    },
    "storage": {
        "commitment": "3_year",
        "payment": "all_upfront",
        "savings": "60%",
        "rationale": "Storage needs are predictable and growing"
    }
}
```

### Data Lifecycle Management

#### S3 Intelligent Tiering
```hcl
# terraform/modules/s3/main.tf
resource "aws_s3_bucket_intelligent_tiering_configuration" "analytics_data" {
  bucket = aws_s3_bucket.analytics_data.id
  name   = "analytics-data-tiering"

  tiering {
    access_tier = "DEEP_ARCHIVE_ACCESS"
    days        = 180
  }

  tiering {
    access_tier = "ARCHIVE_ACCESS"
    days        = 90
  }
}

resource "aws_s3_bucket_lifecycle_configuration" "analytics_data" {
  bucket = aws_s3_bucket.analytics_data.id

  rule {
    id     = "analytics_lifecycle"
    status = "Enabled"

    expiration {
      days = 2555  # 7 years retention for compliance
    }

    noncurrent_version_expiration {
      noncurrent_days = 30
    }

    abort_incomplete_multipart_upload {
      days_after_initiation = 7
    }
  }
}
```

**Cost Impact:**
- Standard to IA: 40% savings after 30 days
- IA to Glacier: 68% savings after 90 days
- Glacier to Deep Archive: 75% savings after 180 days

### Right-Sizing Recommendations

#### Continuous Cost Monitoring
```python
def analyze_resource_utilization():
    """
    Analyze resource utilization and provide right-sizing recommendations.
    Based on CloudWatch metrics and cost allocation tags.
    """
    recommendations = {
        "underutilized_instances": [
            {
                "instance_id": "i-1234567890abcdef0",
                "instance_type": "m5.2xlarge",
                "avg_cpu_utilization": 15,
                "recommended_type": "m5.large",
                "monthly_savings": 145.44
            }
        ],
        "overutilized_instances": [
            {
                "instance_id": "i-abcdef1234567890",
                "instance_type": "m5.large",
                "avg_cpu_utilization": 85,
                "recommended_type": "m5.xlarge",
                "monthly_cost_increase": 72.72
            }
        ],
        "total_monthly_savings": 1250.00
    }
    return recommendations
```

### Multi-Region Cost Optimization

#### Data Transfer Optimization
```yaml
# CloudFront distribution for API responses
cloudfront_config:
  price_class: "PriceClass_100"  # US, Canada, Europe only
  compress: true
  cache_behavior:
    ttl: 300  # 5 minutes for analytics data
    query_string_cache_keys: ["date_range", "aggregation"]
```

**Cost Savings:**
- Reduced data transfer costs by 60%
- Lower API response times globally
- Cached results reduce compute load

## Performance Monitoring and Alerting

### Key Performance Indicators (KPIs)

#### Spark Application Metrics
```yaml
spark_metrics:
  throughput:
    - metric: "records_processed_per_second"
      target: "> 1000"
      alert_threshold: "< 500"
    - metric: "batch_processing_time"
      target: "< 5 minutes"
      alert_threshold: "> 10 minutes"

  resource_utilization:
    - metric: "executor_cpu_utilization"
      target: "60-80%"
      alert_threshold: "> 90% or < 30%"
    - metric: "executor_memory_utilization"
      target: "< 85%"
      alert_threshold: "> 95%"

  data_quality:
    - metric: "failed_tasks_percentage"
      target: "< 1%"
      alert_threshold: "> 5%"
    - metric: "data_skew_ratio"
      target: "< 3:1"
      alert_threshold: "> 10:1"
```

#### API Performance Metrics
```yaml
api_metrics:
  response_times:
    - endpoint: "/api/v1/analytics/revenue"
      p95_target: "< 500ms"
      p99_target: "< 1000ms"
    - endpoint: "/api/v1/customers/segments"
      p95_target: "< 200ms"
      p99_target: "< 500ms"

  throughput:
    - metric: "requests_per_second"
      target: "> 100"
      alert_threshold: "< 50"

  error_rates:
    - metric: "4xx_error_rate"
      target: "< 5%"
      alert_threshold: "> 10%"
    - metric: "5xx_error_rate"
      target: "< 1%"
      alert_threshold: "> 2%"
```

### Automated Performance Alerts

#### Prometheus Alert Rules
```yaml
# monitoring/prometheus/rules/performance.yml
groups:
- name: spark.performance
  rules:
  - alert: SparkJobHighLatency
    expr: spark_streaming_batch_processing_time > 300000  # 5 minutes in ms
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Spark streaming job experiencing high latency"
      description: "Batch processing time {{ $value }}ms exceeds threshold"

  - alert: SparkExecutorMemoryPressure
    expr: spark_executor_memory_used / spark_executor_memory_total > 0.9
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Spark executor memory utilization high"
      description: "Memory utilization {{ $value | humanizePercentage }} on {{ $labels.instance }}"

- name: api.performance
  rules:
  - alert: APIHighResponseTime
    expr: histogram_quantile(0.95, http_request_duration_seconds_bucket{job="api"}) > 0.5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "API response time high"
      description: "95th percentile response time is {{ $value }}s"
```

### Performance Dashboards

#### Grafana Dashboard Configuration
```json
{
  "dashboard": {
    "title": "ECAP Performance Overview",
    "panels": [
      {
        "title": "Spark Streaming Throughput",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(spark_streaming_total_received_records[5m])",
            "legendFormat": "Records/sec"
          }
        ]
      },
      {
        "title": "API Response Times",
        "type": "heatmap",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Resource Utilization",
        "type": "stat",
        "targets": [
          {
            "expr": "avg(spark_executor_cpu_utilization)",
            "legendFormat": "CPU %"
          },
          {
            "expr": "avg(spark_executor_memory_utilization)",
            "legendFormat": "Memory %"
          }
        ]
      }
    ]
  }
}
```

## Troubleshooting Performance Issues

### Common Performance Problems

#### 1. Spark Job Slowdowns

**Symptoms:**
- Increasing batch processing times
- High executor memory usage
- Frequent garbage collection

**Diagnosis Steps:**
```bash
# Check Spark UI for stage timing
curl -s "http://spark-master:4040/api/v1/applications" | jq '.[] | {id, name}'

# Analyze task distribution
curl -s "http://spark-master:4040/api/v1/applications/app-id/stages" | jq '.[] | {stageId, numTasks, executorRunTime}'

# Check for data skew
spark-submit --conf spark.sql.adaptive.skewJoin.enabled=true \
             --conf spark.sql.adaptive.skewJoin.skewedPartitionFactor=5 \
             your_job.py
```

**Common Solutions:**
- Increase executor memory: `--executor-memory 8g`
- Adjust partition count: `df.repartition(optimal_partitions)`
- Enable AQE skew handling: `spark.sql.adaptive.skewJoin.enabled=true`
- Use broadcast joins for small tables: `broadcast(small_df)`

#### 2. API Performance Degradation

**Symptoms:**
- Increasing response times
- High CPU usage on API servers
- Database connection pool exhaustion

**Diagnosis Steps:**
```bash
# Check API metrics
curl -s "http://api:8000/metrics" | grep -E "(response_time|request_count)"

# Database connection status
psql -h postgres -c "SELECT count(*) as active_connections FROM pg_stat_activity WHERE state = 'active';"

# Redis cache hit rate
redis-cli info stats | grep keyspace_hits
```

**Solutions:**
- Scale API horizontally: Increase replica count
- Optimize database queries: Add indexes, use query caching
- Implement Redis caching: Cache frequent query results
- Connection pooling: Increase pool size if needed

#### 3. Memory Issues

**Memory Leak Detection:**
```python
# src/utils/performance_utils.py
import psutil
import logging

def monitor_memory_usage(func):
    """Decorator to monitor memory usage of functions."""
    def wrapper(*args, **kwargs):
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB

        result = func(*args, **kwargs)

        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_diff = memory_after - memory_before

        if memory_diff > 100:  # Log if memory increase > 100MB
            logging.warning(f"{func.__name__} used {memory_diff:.1f}MB memory")

        return result
    return wrapper
```

### Performance Testing Framework

#### Load Testing Configuration
```python
# tests/performance/load_test.py
import asyncio
import aiohttp
import time

async def load_test_api(concurrent_users=50, duration_seconds=300):
    """
    Load test API endpoints with concurrent users.
    """
    results = {
        "total_requests": 0,
        "successful_requests": 0,
        "failed_requests": 0,
        "response_times": []
    }

    async def make_request(session, url):
        start_time = time.time()
        try:
            async with session.get(url) as response:
                await response.text()
                response_time = (time.time() - start_time) * 1000  # ms
                results["response_times"].append(response_time)

                if response.status == 200:
                    results["successful_requests"] += 1
                else:
                    results["failed_requests"] += 1
        except Exception as e:
            results["failed_requests"] += 1

        results["total_requests"] += 1

    # Run load test
    connector = aiohttp.TCPConnector(limit=concurrent_users)
    async with aiohttp.ClientSession(connector=connector) as session:
        start_time = time.time()
        while time.time() - start_time < duration_seconds:
            tasks = []
            for _ in range(concurrent_users):
                url = "http://api:8000/api/v1/analytics/revenue"
                tasks.append(make_request(session, url))

            await asyncio.gather(*tasks)
            await asyncio.sleep(1)  # 1 second between batches

    # Calculate statistics
    response_times = results["response_times"]
    results["avg_response_time"] = sum(response_times) / len(response_times)
    results["p95_response_time"] = sorted(response_times)[int(len(response_times) * 0.95)]
    results["requests_per_second"] = results["total_requests"] / duration_seconds

    return results
```

### Performance Optimization Checklist

#### Pre-Production Checklist
- [ ] **Spark Configuration Optimized**
  - [ ] AQE enabled
  - [ ] Appropriate executor sizing
  - [ ] Kryo serialization enabled
  - [ ] Memory fractions tuned

- [ ] **Database Performance**
  - [ ] Indexes created for frequent queries
  - [ ] Connection pooling configured
  - [ ] Query performance analyzed
  - [ ] Statistics updated

- [ ] **Caching Strategy**
  - [ ] Redis cache warming implemented
  - [ ] Cache invalidation strategy defined
  - [ ] Cache hit rate monitoring enabled
  - [ ] TTL values optimized

- [ ] **API Optimization**
  - [ ] Response compression enabled
  - [ ] Pagination implemented
  - [ ] Rate limiting configured
  - [ ] Connection keep-alive enabled

- [ ] **Infrastructure Scaling**
  - [ ] Auto-scaling policies defined
  - [ ] Resource limits set
  - [ ] Health checks configured
  - [ ] Load balancing optimized

#### Production Monitoring Checklist
- [ ] **Performance Metrics Collection**
  - [ ] Application metrics exported to Prometheus
  - [ ] Custom business metrics tracked
  - [ ] Infrastructure metrics monitored
  - [ ] Log aggregation configured

- [ ] **Alerting Configuration**
  - [ ] Performance threshold alerts set
  - [ ] Escalation procedures defined
  - [ ] Alert fatigue prevention implemented
  - [ ] Runbook documentation created

- [ ] **Capacity Planning**
  - [ ] Growth projections documented
  - [ ] Resource scaling triggers defined
  - [ ] Cost monitoring enabled
  - [ ] Performance baseline established

This performance tuning guide provides comprehensive optimization strategies proven in production deployments of the E-Commerce Analytics Platform. Regular review and updates of these configurations ensure optimal performance as data volumes and usage patterns evolve.
