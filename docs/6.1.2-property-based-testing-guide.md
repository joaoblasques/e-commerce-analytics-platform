# Property-Based Testing Implementation Guide

## Overview

This document provides comprehensive guidance for using property-based testing in the E-Commerce Analytics Platform (ECAP). Property-based testing uses the Hypothesis library to automatically generate test inputs and discover edge cases that traditional unit tests might miss.

## What is Property-Based Testing?

Property-based testing is a testing methodology where you:

1. **Define properties** - Invariants that should always hold true
2. **Generate inputs** - Automatically create diverse test data
3. **Verify properties** - Check that properties hold for all generated inputs
4. **Discover edge cases** - Find unexpected inputs that break your code

## Implementation Components

### 1. Data Transformation Property Tests

**File**: `tests/unit/test_property_based_transformations.py`

**Coverage**:
- Spark streaming transformations
- Data enrichment pipelines
- Business rule validation
- Temporal data processing
- Price categorization logic

**Key Properties Tested**:
- Data integrity preservation (no record loss)
- Column completeness (all expected columns present)
- Value range validation (scores within bounds)
- Monotonic relationships (higher inputs â†’ higher outputs)
- Null value handling

**Example Property**:
```python
@given(st.lists(transaction_data(), min_size=1, max_size=100))
def test_transaction_enrichment_properties(self, transaction_list):
    # Property 1: No data loss
    assert len(results) == len(transaction_list)

    # Property 2: All original columns preserved
    assert original_columns.issubset(enriched_columns)

    # Property 3: Value ranges maintained
    assert 0 <= row["risk_score"] <= 3
```

### 2. API Endpoint Fuzz Testing

**File**: `tests/unit/test_property_based_api.py`

**Coverage**:
- FastAPI endpoint robustness
- Authentication edge cases
- Request validation
- Error handling consistency
- Performance boundaries

**Key Properties Tested**:
- Graceful error handling (no crashes)
- Consistent HTTP status codes
- Valid JSON responses
- Reasonable response times
- Security against malformed inputs

**Example Fuzz Test**:
```python
@given(malformed_json_strings())
def test_api_endpoints_handle_malformed_json(self, malformed_json):
    response = client.post(endpoint, data=malformed_json)

    # Should not crash - must return valid HTTP status
    assert 200 <= response.status_code < 600

    # Response should be valid JSON
    response.json()  # Should not raise JSONDecodeError
```

### 3. Business Rule Invariant Testing

**File**: `tests/unit/test_property_based_business_rules.py`

**Coverage**:
- RFM segmentation logic
- Fraud detection rules
- Revenue calculations
- Customer categorization
- Price tier validation

**Key Properties Tested**:
- Score boundary compliance
- Mathematical invariants (e.g., AOV = Revenue / Transactions)
- Monotonic relationships
- Completeness (all customers assigned segments)
- Precision maintenance (currency calculations)

**Example Invariant**:
```python
@given(st.floats(min_value=0.01, max_value=1000000.0))
def test_revenue_calculation_invariant(self, total_amount):
    # Invariant: Revenue impact should always be 20% of total amount
    expected_revenue = total_amount * 0.2
    calculated_revenue = calculate_revenue_impact(total_amount)

    assert abs(calculated_revenue - expected_revenue) < 0.01
```

### 4. Edge Case Discovery Automation

**File**: `tests/unit/test_edge_case_discovery.py`

**Coverage**:
- Automated boundary value testing
- Extreme input generation
- Error pattern analysis
- Stateful edge case discovery
- Regression test suite generation

**Key Features**:
- Numeric boundary testing (infinity, NaN, overflow)
- String edge cases (empty, unicode, special characters)
- Temporal boundaries (min/max dates, epoch)
- Automated report generation
- Pattern recognition in failures

**Example Discovery**:
```python
def discover_numeric_edge_cases(self, test_function):
    boundary_values = [
        0, float('inf'), float('nan'), sys.maxsize, None
    ]

    for value in boundary_values:
        try:
            result = test_function(value)
            if self._is_suspicious_result(result):
                edge_cases_found.append({
                    'input': value,
                    'output': result,
                    'type': 'suspicious_result'
                })
        except Exception as e:
            edge_cases_found.append({
                'input': value,
                'error': str(e),
                'type': 'exception'
            })
```

## Test Configuration

### Hypothesis Settings

```python
from hypothesis import settings

# Standard test settings
@settings(max_examples=50, deadline=None)

# Intensive testing
@settings(max_examples=200, deadline=None)

# Quick smoke tests
@settings(max_examples=10, deadline=None)
```

### Custom Strategies

**Realistic E-commerce Data**:
```python
@st.composite
def transaction_data(draw):
    return {
        "transaction_id": draw(st.uuids().map(str)),
        "amount": draw(st.floats(min_value=0.01, max_value=10000.0)),
        "customer_tier": draw(st.sampled_from(["premium", "gold", "silver"])),
        "timestamp": draw(st.datetimes(min_value=datetime(2020, 1, 1)))
    }
```

**Extreme Values**:
```python
@st.composite
def extreme_numeric_values(draw):
    return draw(st.sampled_from([
        0, float('inf'), float('nan'), sys.maxsize, 1e-100, 1e100
    ]))
```

## Running Property-Based Tests

### Basic Execution

```bash
# Run all property-based tests
poetry run pytest tests/unit/test_property_based_*.py -v

# Run specific test category
poetry run pytest tests/unit/test_property_based_transformations.py -v

# Run with increased examples
poetry run pytest tests/unit/test_property_based_api.py -v --hypothesis-verbosity=verbose
```

### CI/CD Integration

Property-based tests are integrated into the existing pytest configuration:

```toml
# pyproject.toml
[tool.pytest.ini_options]
addopts = [
    "--hypothesis-show-statistics",
    "--hypothesis-verbosity=normal"
]
```

### Performance Considerations

- **Standard tests**: 10-50 examples (fast feedback)
- **Thorough tests**: 100-200 examples (comprehensive coverage)
- **Continuous testing**: Use `@reproduce_failure` for debugging
- **Parallel execution**: Compatible with pytest-xdist

## Test Categories and Markers

```python
# Mark tests by type
@pytest.mark.property_based
@pytest.mark.fuzz_testing
@pytest.mark.edge_cases
@pytest.mark.business_rules

# Run specific categories
pytest -m property_based
pytest -m "fuzz_testing and not slow"
```

## Debugging Property-Based Tests

### Reproducing Failures

When a property-based test fails, Hypothesis provides reproduction information:

```python
@reproduce_failure('6.136.6', b'...')  # Auto-generated
def test_failing_property(self):
    # Test will reproduce the exact failure
```

### Hypothesis Statistics

```python
from hypothesis import note

@given(st.integers())
def test_with_logging(self, value):
    note(f"Testing with value: {value}")
    # Test logic here
```

### Custom Shrinking

```python
from hypothesis.strategies import composite

@composite
def custom_strategy(draw):
    # Custom generation with better shrinking
    base_value = draw(st.integers(min_value=1))
    return SomeComplexObject(base_value)
```

## Best Practices

### 1. Property Design

**Good Properties**:
- Test invariants, not implementations
- Focus on relationships between inputs/outputs
- Check boundary conditions
- Verify error handling

**Example**:
```python
# Good: Tests the relationship
def test_sorted_list_property(self, input_list):
    sorted_list = sort_function(input_list)
    assert len(sorted_list) == len(input_list)  # No data loss
    assert all(a <= b for a, b in zip(sorted_list, sorted_list[1:]))  # Sorted
```

### 2. Strategy Selection

**Domain-Specific Strategies**:
- Use realistic value ranges for your domain
- Include edge cases that matter to your business
- Generate correlated data when relationships matter

**Example**:
```python
# Domain-aware generation
@st.composite
def customer_data(draw):
    tier = draw(st.sampled_from(["bronze", "silver", "gold", "premium"]))
    # Lifetime value correlates with tier
    if tier == "premium":
        ltv = draw(st.floats(min_value=1000, max_value=50000))
    else:
        ltv = draw(st.floats(min_value=0, max_value=1000))

    return {"tier": tier, "lifetime_value": ltv}
```

### 3. Performance Optimization

**Fast Feedback Loop**:
- Use fewer examples during development
- Increase examples in CI/CD
- Profile slow properties and optimize

**Example**:
```python
# Development: fast feedback
@settings(max_examples=10)
def test_during_development(self):
    pass

# CI: thorough testing
@settings(max_examples=100)
def test_in_ci(self):
    pass
```

### 4. Integration with Existing Tests

**Complement, Don't Replace**:
- Use property-based tests for invariants
- Keep unit tests for specific behaviors
- Use integration tests for workflows

## Troubleshooting Common Issues

### 1. Flaky Tests

**Problem**: Tests pass/fail randomly

**Solutions**:
- Use `assume()` to filter invalid inputs
- Check for floating-point precision issues
- Ensure deterministic test setup

```python
@given(st.floats())
def test_with_assumption(self, value):
    assume(not math.isnan(value))  # Skip NaN values
    assume(abs(value) < 1e10)      # Skip extreme values
    # Test logic here
```

### 2. Slow Test Execution

**Problem**: Property-based tests are too slow

**Solutions**:
- Reduce `max_examples` for development
- Use `@example()` decorator for specific cases
- Profile and optimize expensive operations

```python
@given(st.lists(st.integers(), max_size=100))  # Limit size
@example([])  # Always test empty list
@example([1])  # Always test single item
def test_with_examples(self, input_list):
    # Test logic here
```

### 3. Unclear Failure Messages

**Problem**: Hard to understand what failed

**Solutions**:
- Use `note()` to log intermediate values
- Add descriptive assertion messages
- Use `@reproduce_failure` for debugging

```python
@given(st.integers())
def test_with_clear_messages(self, value):
    note(f"Input value: {value}")
    result = process_value(value)
    note(f"Processed result: {result}")

    assert result >= 0, f"Result {result} should be non-negative for input {value}"
```

## Integration with ECAP Architecture

### 1. Data Pipeline Testing

Property-based tests validate data transformation pipelines:
- Input/output data integrity
- Schema preservation
- Business rule compliance
- Performance characteristics

### 2. API Robustness

Fuzz testing ensures API endpoints handle:
- Malformed requests gracefully
- Authentication edge cases
- Rate limiting scenarios
- Data validation boundaries

### 3. Business Logic Validation

Invariant testing verifies:
- Mathematical relationships
- Business rule consistency
- Domain constraints
- Regulatory compliance

### 4. Quality Assurance Integration

Property-based tests integrate with existing QA processes:
- Run automatically in CI/CD
- Generate test reports
- Track edge case coverage
- Monitor regression patterns

## Metrics and Reporting

### Test Coverage Metrics

- **Property Coverage**: Percentage of business rules tested
- **Edge Case Discovery**: Number of edge cases found
- **Failure Rate**: Percentage of properties that fail
- **Example Efficiency**: Average examples needed to find failures

### Automated Reporting

```python
# Generate edge case report
report = edge_case_tester.generate_edge_case_report()
print(f"Total edge cases discovered: {report['total_edge_cases']}")
print(f"Exception patterns: {report['error_patterns']}")
```

## Future Enhancements

### 1. Machine Learning Integration

- Generate test data using ML models
- Learn from production data patterns
- Adaptive test case generation

### 2. Performance Property Testing

- Memory usage properties
- Execution time bounds
- Scalability characteristics

### 3. Multi-Service Property Testing

- Cross-service invariants
- Distributed system properties
- Event consistency validation

## Conclusion

Property-based testing significantly enhances the robustness of the E-Commerce Analytics Platform by:

1. **Discovering edge cases** that manual testing misses
2. **Validating business rules** across diverse inputs
3. **Ensuring API robustness** against malformed requests
4. **Maintaining data integrity** through transformations
5. **Providing regression protection** against future changes

The implementation provides comprehensive coverage across data transformations, API endpoints, business rules, and edge case discovery, ensuring the platform handles unexpected inputs gracefully and maintains consistent behavior under all conditions.
