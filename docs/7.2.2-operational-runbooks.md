# Operational Runbooks - E-Commerce Analytics Platform
## 24/7 System Operations Guide

### Document Information
- **Document**: Task 7.2.2 - Operational Runbooks
- **Version**: 1.0.0
- **Created**: 2025-07-30
- **Target Audience**: Operations Team, On-Call Engineers, Site Reliability Engineers
- **Classification**: Internal Operations Guide

---

## Table of Contents

1. [Overview](#overview)
2. [Common Operational Procedures](#common-operational-procedures)
3. [Incident Response Playbooks](#incident-response-playbooks)
4. [System Maintenance Procedures](#system-maintenance-procedures)
5. [Backup and Recovery Procedures](#backup-and-recovery-procedures)
6. [Escalation Matrix](#escalation-matrix)
7. [Emergency Contacts](#emergency-contacts)

---

## Overview

This document provides comprehensive operational runbooks for the E-Commerce Analytics Platform (ECAP), enabling 24/7 system management, incident response, and maintenance operations. These procedures are designed to ensure system reliability, minimize downtime, and maintain data integrity across all platform components.

### System Architecture Quick Reference

**Core Components:**
- **API Layer**: FastAPI (Port 8000)
- **Dashboard**: Streamlit (Port 8501)
- **Message Broker**: Apache Kafka (Port 9092)
- **Stream Processing**: Apache Spark (Master: 8080, Workers: 8081-8082)
- **Database**: PostgreSQL (Port 5432)
- **Cache**: Redis (Port 6379)
- **Object Storage**: MinIO (Port 9000)
- **Monitoring**: Prometheus (9090), Grafana (3000), AlertManager (9093)

### Service Dependencies
```
Dashboard ─→ API ─→ Database
            │    ─→ Redis
            │    ─→ Kafka ─→ Spark ─→ MinIO
            └─→ Prometheus ─→ Grafana
                           ─→ AlertManager
```

---

## Common Operational Procedures

### 1. System Health Monitoring

#### Daily Health Check Routine
```bash
#!/bin/bash
# Execute: ./scripts/daily-health-check.sh

echo "=== ECAP Daily Health Check - $(date) ==="

# Core services status
echo "1. Core Services Status:"
services=("api:8000" "dashboard:8501" "postgres:5432" "kafka:9092" "redis:6379" "minio:9000")
for service in "${services[@]}"; do
    IFS=':' read -r host port <<< "$service"
    if nc -z "$host" "$port" 2>/dev/null; then
        echo "  ✅ $service - UP"
    else
        echo "  ❌ $service - DOWN - REQUIRES IMMEDIATE ATTENTION"
    fi
done

# Spark cluster status
echo "2. Spark Cluster Status:"
spark_services=("spark-master:8080" "spark-worker-1:8081" "spark-worker-2:8082")
for service in "${spark_services[@]}"; do
    IFS=':' read -r host port <<< "$service"
    if nc -z "$host" "$port" 2>/dev/null; then
        echo "  ✅ $service - UP"
    else
        echo "  ❌ $service - DOWN - Check Spark cluster"
    fi
done

# Monitoring services
echo "3. Monitoring Services:"
monitoring=("prometheus:9090" "grafana:3000" "alertmanager:9093")
for service in "${monitoring[@]}"; do
    IFS=':' read -r host port <<< "$service"
    if nc -z "$host" "$port" 2>/dev/null; then
        echo "  ✅ $service - UP"
    else
        echo "  ⚠️ $service - DOWN - Monitoring impacted"
    fi
done

# Data pipeline health
echo "4. Data Pipeline Health:"
# Check Kafka topics
kafka_health=$(docker exec ecap-kafka kafka-topics --bootstrap-server localhost:9092 --list 2>/dev/null | wc -l)
if [ "$kafka_health" -gt 0 ]; then
    echo "  ✅ Kafka topics: $kafka_health topics active"
else
    echo "  ❌ Kafka topics: No topics found - Pipeline stopped"
fi

# Database connectivity
db_health=$(docker exec ecap-postgres pg_isready -U ecap_user -d ecommerce_analytics 2>/dev/null)
if [[ $db_health == *"accepting connections"* ]]; then
    echo "  ✅ PostgreSQL: Database accepting connections"
else
    echo "  ❌ PostgreSQL: Database connection failed"
fi

# Redis connectivity
redis_health=$(docker exec ecap-redis redis-cli -a redis_password ping 2>/dev/null)
if [[ $redis_health == "PONG" ]]; then
    echo "  ✅ Redis: Cache responding"
else
    echo "  ❌ Redis: Cache not responding"
fi

echo "=== Health Check Complete ==="
```

#### Real-Time Service Monitoring
```bash
# Continuous service monitoring
watch -n 30 'docker-compose ps | grep -E "(Up|Exit|Restarting)"'

# Monitor system resources
watch -n 10 'docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"'

# Check application logs in real-time
docker-compose logs -f --tail=100 api dashboard spark-master
```

### 2. Service Management Operations

#### Service Restart Procedures

**Individual Service Restart:**
```bash
# Safe service restart with dependency awareness
restart_service() {
    local service=$1
    echo "Restarting $service..."

    case $service in
        "api")
            docker-compose restart api
            sleep 10
            curl -f http://localhost:8000/health || echo "⚠️ API health check failed"
            ;;
        "dashboard")
            docker-compose restart dashboard
            sleep 15
            curl -f http://localhost:8501 || echo "⚠️ Dashboard not responding"
            ;;
        "kafka")
            echo "⚠️ Kafka restart will affect data pipeline"
            read -p "Continue? (y/N): " confirm
            if [[ $confirm == [yY] ]]; then
                docker-compose restart kafka
                sleep 30
                docker exec ecap-kafka kafka-topics --bootstrap-server localhost:9092 --list
            fi
            ;;
        "postgres")
            echo "⚠️ Database restart will affect all services"
            read -p "Continue? (y/N): " confirm
            if [[ $confirm == [yY] ]]; then
                docker-compose restart postgres
                sleep 20
                docker exec ecap-postgres pg_isready -U ecap_user -d ecommerce_analytics
            fi
            ;;
        "spark-master")
            docker-compose restart spark-master
            sleep 15
            docker-compose restart spark-worker-1 spark-worker-2
            ;;
        *)
            docker-compose restart "$service"
            ;;
    esac
}

# Usage: restart_service api
```

**Graceful System Shutdown:**
```bash
# Graceful shutdown procedure
graceful_shutdown() {
    echo "Initiating graceful system shutdown..."

    # Step 1: Stop data producers
    echo "1. Stopping data producers..."
    docker-compose stop producers

    # Step 2: Allow processing to complete
    echo "2. Waiting for processing to complete (60s)..."
    sleep 60

    # Step 3: Stop streaming services
    echo "3. Stopping Spark streaming..."
    docker-compose stop spark-worker-1 spark-worker-2 spark-master

    # Step 4: Stop API and Dashboard
    echo "4. Stopping API and Dashboard..."
    docker-compose stop api dashboard

    # Step 5: Stop Kafka (after consumers are done)
    echo "5. Stopping Kafka..."
    docker-compose stop kafka zookeeper

    # Step 6: Stop remaining services
    echo "6. Stopping remaining services..."
    docker-compose stop postgres redis minio

    # Step 7: Stop monitoring
    echo "7. Stopping monitoring services..."
    docker-compose stop prometheus grafana alertmanager

    echo "✅ Graceful shutdown complete"
}
```

### 3. Performance Monitoring

#### System Performance Checks
```bash
# Performance monitoring script
check_performance() {
    echo "=== Performance Metrics - $(date) ==="

    # API Response Times
    echo "1. API Performance:"
    api_response=$(curl -w "%{time_total}" -s -o /dev/null http://localhost:8000/health)
    if (( $(echo "$api_response < 1.0" | bc -l) )); then
        echo "  ✅ API Response: ${api_response}s (Good)"
    else
        echo "  ⚠️ API Response: ${api_response}s (Slow - investigate)"
    fi

    # Database Performance
    echo "2. Database Performance:"
    db_connections=$(docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -t -c "SELECT count(*) FROM pg_stat_activity;")
    echo "  Active connections: $db_connections"

    # Kafka Lag Monitoring
    echo "3. Kafka Consumer Lag:"
    docker exec ecap-kafka kafka-consumer-groups --bootstrap-server localhost:9092 --describe --all-groups

    # Memory Usage
    echo "4. Memory Usage:"
    docker stats --no-stream --format "{{.Container}}: {{.MemUsage}}" | head -10

    # Disk Usage
    echo "5. Disk Usage:"
    df -h | grep -E "(docker|var)"
}
```

### 4. Data Pipeline Operations

#### Kafka Topic Management
```bash
# Kafka operations
kafka_ops() {
    local operation=$1

    case $operation in
        "list-topics")
            docker exec ecap-kafka kafka-topics --bootstrap-server localhost:9092 --list
            ;;
        "topic-info")
            local topic=$2
            docker exec ecap-kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic "$topic"
            ;;
        "consumer-groups")
            docker exec ecap-kafka kafka-consumer-groups --bootstrap-server localhost:9092 --list
            ;;
        "reset-offsets")
            local group=$2
            local topic=$3
            echo "⚠️ This will reset consumer offsets - data may be reprocessed"
            read -p "Continue? (y/N): " confirm
            if [[ $confirm == [yY] ]]; then
                docker exec ecap-kafka kafka-consumer-groups --bootstrap-server localhost:9092 --group "$group" --reset-offsets --to-earliest --topic "$topic" --execute
            fi
            ;;
    esac
}
```

#### Spark Job Monitoring
```bash
# Spark monitoring operations
spark_monitoring() {
    echo "=== Spark Cluster Status ==="

    # Cluster overview
    echo "1. Cluster Overview:"
    curl -s http://localhost:8080/json/ | python3 -m json.tool | grep -E "(status|workers|cores|memory)"

    # Active applications
    echo "2. Active Applications:"
    curl -s http://localhost:8080/json/applications/ | python3 -m json.tool

    # Worker status
    echo "3. Worker Status:"
    for port in 8081 8082; do
        worker_status=$(curl -s http://localhost:${port}/json/ 2>/dev/null)
        if [ $? -eq 0 ]; then
            echo "  ✅ Worker on port $port: Active"
        else
            echo "  ❌ Worker on port $port: Unreachable"
        fi
    done
}
```

---

## Incident Response Playbooks

### Severity Classification

**Critical (P0) - Immediate Response Required:**
- Complete system outage
- Data loss or corruption
- Security breach
- Payment processing failure

**High (P1) - Response within 1 hour:**
- Major component failure
- Significant performance degradation
- API response time > 5 seconds
- Database connectivity issues

**Medium (P2) - Response within 4 hours:**
- Minor component failure
- Moderate performance impact
- Non-critical feature unavailable
- Monitoring alerts

**Low (P3) - Response within 24 hours:**
- Documentation issues
- Enhancement requests
- Cosmetic bugs

### Incident Response Workflow

#### P0/P1 Incident Response
```bash
#!/bin/bash
# Critical incident response script
critical_incident_response() {
    local incident_type=$1

    echo "🚨 CRITICAL INCIDENT RESPONSE INITIATED"
    echo "Incident Type: $incident_type"
    echo "Timestamp: $(date)"

    # Step 1: Immediate assessment
    echo "Step 1: System Assessment"
    ./scripts/daily-health-check.sh > /tmp/incident-health-check.log

    # Step 2: Notifications
    echo "Step 2: Notifications"
    echo "- Notify on-call engineer"
    echo "- Create incident channel"
    echo "- Update status page"

    # Step 3: Initial diagnosis
    echo "Step 3: Initial Diagnosis"
    case $incident_type in
        "api-down")
            curl -I http://localhost:8000/health
            docker logs ecap-api --tail=50
            ;;
        "database-down")
            docker exec ecap-postgres pg_isready -U ecap_user -d ecommerce_analytics
            docker logs ecap-postgres --tail=50
            ;;
        "kafka-down")
            docker exec ecap-kafka kafka-topics --bootstrap-server localhost:9092 --list
            docker logs ecap-kafka --tail=50
            ;;
    esac

    # Step 4: Containment actions
    echo "Step 4: Containment Actions"
    echo "- Isolate affected components"
    echo "- Prevent further impact"
    echo "- Preserve evidence"

    echo "📋 Next: Execute specific playbook for $incident_type"
}
```

### Specific Incident Playbooks

#### API Service Outage
```bash
api_outage_playbook() {
    echo "🔧 API OUTAGE PLAYBOOK"

    # Diagnosis
    echo "1. API Health Check:"
    api_status=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/health)
    echo "HTTP Status: $api_status"

    if [ "$api_status" != "200" ]; then
        echo "2. Container Status:"
        docker ps | grep ecap-api

        echo "3. Recent Logs:"
        docker logs ecap-api --tail=100

        echo "4. Resource Usage:"
        docker stats ecap-api --no-stream

        # Recovery Actions
        echo "5. Recovery Actions:"
        echo "   a. Restart API service:"
        echo "      docker-compose restart api"
        echo "   b. Check dependencies:"
        echo "      - Database connectivity"
        echo "      - Redis connectivity"
        echo "   c. Scale if needed:"
        echo "      docker-compose up -d --scale api=2"

        # Verification
        echo "6. Verification:"
        echo "   - Health endpoint returns 200"
        echo "   - Sample API requests work"
        echo "   - Response times < 1 second"
    fi
}
```

#### Database Connection Issues
```bash
database_issue_playbook() {
    echo "🔧 DATABASE ISSUE PLAYBOOK"

    # Diagnosis
    echo "1. Database Connectivity:"
    db_status=$(docker exec ecap-postgres pg_isready -U ecap_user -d ecommerce_analytics 2>&1)
    echo "$db_status"

    echo "2. Connection Count:"
    connections=$(docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -t -c "SELECT count(*) FROM pg_stat_activity;" 2>/dev/null)
    echo "Active connections: ${connections:-"Unable to connect"}"

    echo "3. Database Logs:"
    docker logs ecap-postgres --tail=50

    # Recovery Actions
    echo "4. Recovery Actions:"
    if [[ $db_status == *"no response"* ]]; then
        echo "   a. Database appears down - restart required"
        echo "      docker-compose restart postgres"
    elif [ "${connections:-0}" -gt 90 ]; then
        echo "   a. Too many connections - terminate idle sessions"
        echo "      SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state='idle' AND state_change < now() - interval '1 hour';"
    fi

    echo "5. Verification:"
    echo "   - Connection successful"
    echo "   - Query response time normal"
    echo "   - All dependent services functioning"
}
```

#### Kafka/Streaming Pipeline Issues
```bash
kafka_pipeline_playbook() {
    echo "🔧 KAFKA PIPELINE PLAYBOOK"

    # Diagnosis
    echo "1. Kafka Broker Status:"
    kafka_status=$(docker exec ecap-kafka kafka-topics --bootstrap-server localhost:9092 --list 2>&1)
    if [[ $kafka_status == *"Exception"* ]]; then
        echo "❌ Kafka broker unreachable"
        docker logs ecap-kafka --tail=50
    else
        echo "✅ Kafka broker responding"
        echo "Topics: $(echo "$kafka_status" | wc -l)"
    fi

    echo "2. Consumer Group Status:"
    docker exec ecap-kafka kafka-consumer-groups --bootstrap-server localhost:9092 --describe --all-groups

    echo "3. Spark Streaming Status:"
    spark_apps=$(curl -s http://localhost:8080/json/applications/ | python3 -c "import sys,json; apps=json.load(sys.stdin); print(f'Active: {len([a for a in apps if a[\"state\"]==\"RUNNING\"])}, Total: {len(apps)}')" 2>/dev/null)
    echo "Spark Applications: ${spark_apps:-"Unable to connect to Spark"}"

    # Recovery Actions
    echo "4. Recovery Actions:"
    if [[ $kafka_status == *"Exception"* ]]; then
        echo "   a. Restart Kafka cluster:"
        echo "      docker-compose restart zookeeper"
        echo "      sleep 30"
        echo "      docker-compose restart kafka"
    fi

    echo "   b. Restart Spark streaming jobs:"
    echo "      docker-compose restart spark-master spark-worker-1 spark-worker-2"

    echo "5. Data Integrity Check:"
    echo "   - Verify no data loss during restart"
    echo "   - Check consumer lag after recovery"
    echo "   - Validate downstream data processing"
}
```

### Escalation Procedures

#### Escalation Matrix
```
Level 1: On-Call Engineer (Response: Immediate)
├── Incidents: P0, P1 during business hours
├── Authority: Service restarts, basic troubleshooting
└── Escalate to Level 2 if: Cannot resolve in 30 minutes

Level 2: Senior Engineer/Team Lead (Response: 15 minutes)
├── Incidents: P0 anytime, P1 if Level 1 escalates
├── Authority: System changes, rollbacks, infrastructure modifications
└── Escalate to Level 3 if: Requires architectural changes

Level 3: Engineering Manager/Architect (Response: 30 minutes)
├── Incidents: P0 with business impact, security breaches
├── Authority: Major system changes, vendor engagement
└── Escalate to Level 4 if: Requires business decisions

Level 4: CTO/Business Leadership (Response: 1 hour)
├── Incidents: Major business impact, data breaches, compliance issues
├── Authority: All decisions, external communications
└── Final escalation level
```

---

## System Maintenance Procedures

### Scheduled Maintenance Windows

#### Weekly Maintenance (Sundays 02:00-04:00 UTC)
```bash
weekly_maintenance() {
    echo "🔧 WEEKLY MAINTENANCE STARTING - $(date)"

    # Pre-maintenance checks
    echo "1. Pre-maintenance System Health:"
    ./scripts/daily-health-check.sh

    # Database maintenance
    echo "2. Database Maintenance:"
    docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -c "VACUUM ANALYZE;"
    docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -c "REINDEX DATABASE ecommerce_analytics;"

    # Log rotation
    echo "3. Log Rotation:"
    docker exec ecap-api find /app/logs -name "*.log" -mtime +7 -delete
    docker logs ecap-api > /tmp/api-logs-$(date +%Y%m%d).log

    # Cache cleanup
    echo "4. Cache Cleanup:"
    docker exec ecap-redis redis-cli -a redis_password FLUSHDB

    # Disk cleanup
    echo "5. Disk Cleanup:"
    docker system prune -f

    # Update system metrics
    echo "6. System Metrics Update:"
    ./scripts/collect-weekly-metrics.sh

    # Post-maintenance verification
    echo "7. Post-maintenance Verification:"
    ./scripts/daily-health-check.sh

    echo "✅ WEEKLY MAINTENANCE COMPLETED - $(date)"
}
```

#### Monthly Maintenance (First Sunday 01:00-05:00 UTC)
```bash
monthly_maintenance() {
    echo "🔧 MONTHLY MAINTENANCE STARTING - $(date)"

    # All weekly maintenance tasks
    weekly_maintenance

    # Security updates
    echo "8. Security Updates:"
    docker-compose pull
    docker-compose up -d

    # Certificate renewals
    echo "9. Certificate Check:"
    # Check SSL certificates expiration
    echo "Checking SSL certificates..."

    # Backup verification
    echo "10. Backup Verification:"
    ./scripts/verify-backups.sh

    # Performance optimization
    echo "11. Performance Optimization:"
    docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -c "SELECT * FROM pg_stat_user_tables WHERE n_tup_ins + n_tup_upd + n_tup_del > 1000;"

    # Monitoring cleanup
    echo "12. Monitoring Data Cleanup:"
    # Cleanup old Prometheus data (>30 days)
    docker exec ecap-prometheus rm -rf /prometheus/01*

    echo "✅ MONTHLY MAINTENANCE COMPLETED - $(date)"
}
```

### Component-Specific Maintenance

#### PostgreSQL Maintenance
```bash
postgres_maintenance() {
    echo "🗄️ PostgreSQL Maintenance"

    # Connection limit check
    echo "1. Connection Analysis:"
    docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -c "
        SELECT state, count(*)
        FROM pg_stat_activity
        GROUP BY state;
    "

    # Table statistics
    echo "2. Table Statistics:"
    docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -c "
        SELECT schemaname, tablename, n_tup_ins, n_tup_upd, n_tup_del, n_live_tup, n_dead_tup
        FROM pg_stat_user_tables
        ORDER BY n_dead_tup DESC
        LIMIT 10;
    "

    # Index usage
    echo "3. Index Usage Analysis:"
    docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -c "
        SELECT schemaname, tablename, indexname, idx_tup_read, idx_tup_fetch
        FROM pg_stat_user_indexes
        WHERE idx_tup_read = 0
        ORDER BY schemaname, tablename;
    "

    # Maintenance operations
    echo "4. Maintenance Operations:"
    echo "   - VACUUM ANALYZE (weekly)"
    echo "   - REINDEX (monthly)"
    echo "   - UPDATE STATISTICS (weekly)"
}
```

#### Kafka Maintenance
```bash
kafka_maintenance() {
    echo "📨 Kafka Maintenance"

    # Topic information
    echo "1. Topic Health:"
    for topic in transactions user-events product-updates fraud-alerts analytics-results; do
        echo "Topic: $topic"
        docker exec ecap-kafka kafka-log-dirs --bootstrap-server localhost:9092 --topic-list "$topic" --describe
    done

    # Consumer group lag
    echo "2. Consumer Group Analysis:"
    docker exec ecap-kafka kafka-consumer-groups --bootstrap-server localhost:9092 --describe --all-groups

    # Log retention cleanup
    echo "3. Log Retention:"
    echo "   - Current retention: 7 days"
    echo "   - Log size monitoring in progress"

    # Partition rebalancing (if needed)
    echo "4. Partition Status:"
    docker exec ecap-kafka kafka-topics --bootstrap-server localhost:9092 --describe
}
```

#### Redis Maintenance
```bash
redis_maintenance() {
    echo "🔴 Redis Maintenance"

    # Memory usage
    echo "1. Memory Analysis:"
    docker exec ecap-redis redis-cli -a redis_password info memory

    # Key analysis
    echo "2. Key Statistics:"
    docker exec ecap-redis redis-cli -a redis_password info keyspace

    # Slow queries
    echo "3. Slow Query Log:"
    docker exec ecap-redis redis-cli -a redis_password slowlog get 10

    # Cache hit ratio
    echo "4. Performance Metrics:"
    docker exec ecap-redis redis-cli -a redis_password info stats | grep -E "(keyspace_hits|keyspace_misses)"
}
```

---

## Backup and Recovery Procedures

### Backup Strategy Overview

**Backup Schedule:**
- **Continuous**: Transaction logs, Kafka topics
- **Hourly**: Redis snapshots, application logs
- **Daily**: Full database backup, configuration files
- **Weekly**: System state backup, monitoring data
- **Monthly**: Complete system backup, long-term archival

### Database Backup Procedures

#### PostgreSQL Backup
```bash
#!/bin/bash
postgres_backup() {
    local backup_type=${1:-"daily"}
    local timestamp=$(date +"%Y%m%d_%H%M%S")
    local backup_dir="/backup/postgres"

    mkdir -p "$backup_dir"

    echo "🗄️ PostgreSQL Backup Starting - Type: $backup_type"

    case $backup_type in
        "full")
            echo "Creating full database backup..."
            docker exec ecap-postgres pg_dumpall -U ecap_user > "$backup_dir/full_backup_$timestamp.sql"
            ;;
        "daily")
            echo "Creating daily database backup..."
            docker exec ecap-postgres pg_dump -U ecap_user -d ecommerce_analytics > "$backup_dir/daily_backup_$timestamp.sql"
            ;;
        "schema")
            echo "Creating schema-only backup..."
            docker exec ecap-postgres pg_dump -U ecap_user -d ecommerce_analytics --schema-only > "$backup_dir/schema_backup_$timestamp.sql"
            ;;
    esac

    # Compress backup
    gzip "$backup_dir/*_backup_$timestamp.sql"

    # Verify backup
    if [ -f "$backup_dir/*_backup_$timestamp.sql.gz" ]; then
        echo "✅ Backup completed: $backup_dir/*_backup_$timestamp.sql.gz"

        # Upload to S3/MinIO
        echo "Uploading to object storage..."
        docker run --rm -v "$backup_dir:/backup" \
               minio/mc cp "/backup/*_backup_$timestamp.sql.gz" minio/backups/postgres/

        # Cleanup old backups (keep 30 days)
        find "$backup_dir" -name "*.sql.gz" -mtime +30 -delete

    else
        echo "❌ Backup failed"
        return 1
    fi
}
```

#### Kafka Backup
```bash
kafka_backup() {
    local timestamp=$(date +"%Y%m%d_%H%M%S")
    local backup_dir="/backup/kafka"

    mkdir -p "$backup_dir"

    echo "📨 Kafka Backup Starting"

    # Export topic configurations
    echo "1. Backing up topic configurations..."
    for topic in transactions user-events product-updates fraud-alerts analytics-results; do
        docker exec ecap-kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic "$topic" > "$backup_dir/topic_config_${topic}_$timestamp.txt"
    done

    # Consumer group offsets
    echo "2. Backing up consumer group offsets..."
    docker exec ecap-kafka kafka-consumer-groups --bootstrap-server localhost:9092 --describe --all-groups > "$backup_dir/consumer_groups_$timestamp.txt"

    # For critical topics, create offset snapshots
    echo "3. Creating offset snapshots..."
    for topic in transactions fraud-alerts; do
        docker exec ecap-kafka kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic "$topic" > "$backup_dir/offsets_${topic}_$timestamp.txt"
    done

    # Compress and upload
    tar -czf "$backup_dir/kafka_backup_$timestamp.tar.gz" "$backup_dir"/*_$timestamp.txt
    docker run --rm -v "$backup_dir:/backup" \
           minio/mc cp "/backup/kafka_backup_$timestamp.tar.gz" minio/backups/kafka/

    echo "✅ Kafka backup completed"
}
```

### Recovery Procedures

#### Database Recovery
```bash
postgres_recovery() {
    local backup_file=$1
    local recovery_type=${2:-"full"}

    echo "🔄 PostgreSQL Recovery Starting"
    echo "Backup file: $backup_file"
    echo "Recovery type: $recovery_type"

    # Safety confirmation
    echo "⚠️ This will overwrite the current database"
    read -p "Continue with recovery? (yes/no): " confirm
    if [ "$confirm" != "yes" ]; then
        echo "Recovery cancelled"
        return 1
    fi

    # Stop dependent services
    echo "1. Stopping dependent services..."
    docker-compose stop api dashboard

    # Create recovery point
    echo "2. Creating recovery point..."
    docker exec ecap-postgres pg_dump -U ecap_user -d ecommerce_analytics > "/tmp/pre_recovery_$(date +%Y%m%d_%H%M%S).sql"

    # Perform recovery
    echo "3. Performing recovery..."
    case $recovery_type in
        "full")
            docker exec ecap-postgres dropdb -U ecap_user ecommerce_analytics
            docker exec ecap-postgres createdb -U ecap_user ecommerce_analytics
            docker exec -i ecap-postgres psql -U ecap_user -d ecommerce_analytics < "$backup_file"
            ;;
        "data-only")
            docker exec -i ecap-postgres psql -U ecap_user -d ecommerce_analytics -c "TRUNCATE TABLE transactions, user_events, analytics_results CASCADE;"
            docker exec -i ecap-postgres psql -U ecap_user -d ecommerce_analytics < "$backup_file"
            ;;
    esac

    # Verify recovery
    echo "4. Verifying recovery..."
    db_status=$(docker exec ecap-postgres psql -U ecap_user -d ecommerce_analytics -c "SELECT count(*) FROM information_schema.tables WHERE table_schema='public';" -t)
    echo "Tables recovered: $db_status"

    # Restart services
    echo "5. Restarting services..."
    docker-compose start api dashboard

    # Final verification
    echo "6. Final verification..."
    ./scripts/daily-health-check.sh

    echo "✅ Recovery completed"
}
```

#### System Recovery
```bash
disaster_recovery() {
    local scenario=$1

    echo "🚨 DISASTER RECOVERY PROCEDURE"
    echo "Scenario: $scenario"
    echo "Timestamp: $(date)"

    case $scenario in
        "complete-failure")
            echo "Complete system failure recovery:"
            echo "1. Restore from infrastructure backup"
            echo "2. Recreate Docker network: docker network create ecap-network"
            echo "3. Restore data volumes"
            echo "4. Start services in order:"
            echo "   - postgres, redis, minio"
            echo "   - kafka, zookeeper"
            echo "   - spark-master, spark-workers"
            echo "   - api, dashboard"
            echo "   - monitoring services"
            ;;
        "data-corruption")
            echo "Data corruption recovery:"
            echo "1. Identify corrupted components"
            echo "2. Stop affected services"
            echo "3. Restore from latest backup"
            echo "4. Validate data integrity"
            echo "5. Resume operations"
            ;;
        "security-breach")
            echo "Security breach response:"
            echo "1. Immediate isolation"
            echo "2. Change all credentials"
            echo "3. System forensics"
            echo "4. Clean restore"
            echo "5. Security hardening"
            ;;
    esac

    echo "📋 Execute appropriate detailed recovery plan"
}
```

### Backup Verification

#### Automated Backup Testing
```bash
verify_backups() {
    echo "🔍 Backup Verification Starting"

    # PostgreSQL backup verification
    echo "1. PostgreSQL Backup Verification:"
    latest_backup=$(ls -t /backup/postgres/*.sql.gz | head -1)
    if [ -f "$latest_backup" ]; then
        echo "✅ Latest backup found: $latest_backup"

        # Test restore in isolated environment
        echo "Testing backup integrity..."
        gunzip -c "$latest_backup" | head -10

        # Verify backup size (should be > 1MB for non-empty database)
        backup_size=$(stat -f%z "$latest_backup" 2>/dev/null || stat -c%s "$latest_backup")
        if [ "$backup_size" -gt 1048576 ]; then
            echo "✅ Backup size OK: $(($backup_size / 1024 / 1024))MB"
        else
            echo "⚠️ Backup size suspicious: $(($backup_size / 1024))KB"
        fi
    else
        echo "❌ No PostgreSQL backup found"
    fi

    # Kafka backup verification
    echo "2. Kafka Backup Verification:"
    latest_kafka_backup=$(ls -t /backup/kafka/*.tar.gz | head -1)
    if [ -f "$latest_kafka_backup" ]; then
        echo "✅ Latest Kafka backup found: $latest_kafka_backup"
        tar -tzf "$latest_kafka_backup" | head -5
    else
        echo "❌ No Kafka backup found"
    fi

    # Object storage verification
    echo "3. Object Storage Verification:"
    docker run --rm minio/mc ls minio/backups/ | tail -10

    echo "✅ Backup verification completed"
}
```

---

## Escalation Matrix

### Contact Information

#### Primary Contacts
```
On-Call Engineer (Level 1)
├── Response Time: Immediate
├── Availability: 24/7
├── Contact: Slack @oncall, Phone: +1-XXX-XXX-XXXX
└── Backup: Slack @oncall-backup

Senior Engineer (Level 2)
├── Response Time: 15 minutes
├── Availability: Business hours + on-call rotation
├── Contact: Slack @senior-eng, Phone: +1-XXX-XXX-XXXX
└── Backup: Slack @senior-eng-backup

Engineering Manager (Level 3)
├── Response Time: 30 minutes
├── Availability: Business hours + emergency
├── Contact: Slack @eng-manager, Phone: +1-XXX-XXX-XXXX
└── Backup: Slack @tech-lead

CTO/Business (Level 4)
├── Response Time: 1 hour
├── Availability: Emergency only
├── Contact: Slack @cto, Phone: +1-XXX-XXX-XXXX
└── Alternative: Slack @business-owner
```

#### Vendor Contacts
```
Infrastructure Vendor
├── Support Level: 24/7 Enterprise
├── Contact: support@vendor.com
├── Phone: +1-XXX-XXX-XXXX
└── Ticket System: https://vendor.com/support

Database Vendor (PostgreSQL Support)
├── Support Level: Business hours
├── Contact: postgres-support@vendor.com
├── Phone: +1-XXX-XXX-XXXX
└── Documentation: https://postgresql.org/docs

Monitoring Vendor
├── Support Level: 24/7
├── Contact: monitoring-support@vendor.com
├── Phone: +1-XXX-XXX-XXXX
└── Status Page: https://vendor.statuspage.io
```

---

## Emergency Contacts

### Emergency Response Team
```
Incident Commander: [Name] - Slack @incident-commander - Mobile: +1-XXX-XXX-XXXX
Technical Lead: [Name] - Slack @tech-lead - Mobile: +1-XXX-XXX-XXXX
Communications Lead: [Name] - Slack @comms-lead - Mobile: +1-XXX-XXX-XXXX
Business Lead: [Name] - Slack @business-lead - Mobile: +1-XXX-XXX-XXXX
```

### Communication Channels
```
Primary: Slack #incidents
Secondary: Slack #ops-alerts
Emergency: Conference Bridge: dial-in-number
Status Updates: #status-updates
External: status.company.com
```

### Documentation and References

#### Quick Reference Links
- System Architecture: [Internal Wiki Link]
- API Documentation: http://localhost:8000/docs
- Monitoring Dashboards: http://localhost:3000
- Log Aggregation: [ELK Stack URL]
- Service Status: [Status Page URL]

#### Standard Operating Procedures
- Change Management Process: [Link]
- Incident Response Process: [Link]
- Security Incident Response: [Link]
- Data Breach Response: [Link]
- Business Continuity Plan: [Link]

---

## Conclusion

This operational runbook provides comprehensive procedures for maintaining the E-Commerce Analytics Platform in a 24/7 operational environment. Regular updates to these procedures should be made based on:

- System changes and upgrades
- Lessons learned from incidents
- Changes in business requirements
- Regulatory compliance updates
- Technology stack evolution

**Review Schedule:**
- Monthly: Review and update contact information
- Quarterly: Review and test all procedures
- Semi-annually: Full runbook review and revision
- Annually: Complete operational readiness assessment

**Training Requirements:**
- All operations team members must be familiar with these procedures
- Regular tabletop exercises should be conducted
- New team members must complete runbook training within 30 days
- Emergency response drills should be conducted quarterly

For questions or updates to this runbook, contact the Operations Team or create a ticket in the internal ticketing system.

---

*Document Version: 1.0.0 | Last Updated: 2025-07-30 | Next Review: 2025-10-30*
